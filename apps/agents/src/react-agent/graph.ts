import { BaseMessage, HumanMessage, SystemMessage, AIMessage, isSystemMessage, isHumanMessage, isAIMessage, isToolMessage, ToolMessage } from "@langchain/core/messages";
import { Annotation, Command, END, MemorySaver, START, StateGraph, interrupt, messagesStateReducer, type LangGraphRunnableConfig } from "@langchain/langgraph";
import { JsonOutputParser } from "@langchain/core/output_parsers";
import { ConfigurationSchema } from "./configuration.js";
import { typedUi, uiMessageReducer } from "@langchain/langgraph-sdk/react-ui/server";
import { loadChatModel, loadModal } from "./utils.js";
import { AnalysisOfIntentionsPrompt, ExtractParametersPrompt, AnalysisAgentPrompt } from "./prompts/fetch-agent.js";
import { apis } from "./config/apis.js";
import * as z from 'zod'
import { ApiExecutor } from "./utils/apiExecutor.js";
import { createReactAgent } from "@langchain/langgraph/prebuilt";
import { getMarketingPlan, ChartTools } from "./tools/index.js";
import { MultiServerMCPClient } from "@langchain/mcp-adapters";
import { v4 as uuidv4 } from 'uuid';

const chatModel = loadChatModel();

const analysisModal = loadModal()

interface AnalysisOfIntentionsOutput {
  user_intent: string;
  analysis_details: string;
  matched_apis: {
    api_name: string;
    relevance_score: number;
    match_reason: string;
  }[];
  primary_recommendation: string;
  api_params: Record<string, any>;
  confidence_level: "high" | "medium" | "low";
}

const structuredOutput = z.object({
  user_intent: z.string().describe("ç”¨æˆ·æ„å›¾çš„ç®€æ´æè¿°"),
  analysis_details: z.string().describe("è¯¦ç»†çš„éœ€æ±‚åˆ†æè¿‡ç¨‹"),
  matched_apis: z.array(z.object({
    api_name: z.string().describe("åŒ¹é…çš„APIåç§°"),
    relevance_score: z.number().describe("ç›¸å…³æ€§è¯„åˆ†(1-10)"),
    match_reason: z.string().describe("åŒ¹é…åŸå› è¯´æ˜"),
  })).describe("åŒ¹é…çš„APIåˆ—è¡¨"),
  primary_recommendation: z.string().describe("æœ€ä¸»è¦æ¨èçš„APIåç§°"),
  api_params: z.record(z.string(), z.any()).describe("APIå‚æ•°"),
  confidence_level: z.enum(["high", "medium", "low"]).describe("åŒ¹é…ç½®ä¿¡åº¦"),
})


// Graph state
const StateAnnotation = Annotation.Root({
  messages: Annotation<BaseMessage[]>({
    reducer: messagesStateReducer,
    default: () => [],
  }),
  apiInfo: Annotation<AnalysisOfIntentionsOutput>,
  apiResult: Annotation<any>,
  secondExtract: Annotation<{
    sourceState: AnalysisOfIntentionsOutput['api_params']
    schema: z.ZodObject<any>
    userInput: string
  }>,
  thoughtChain: Annotation<{
    key: 'analysisOfIntentions' | 'matchApis' | 'apiExecutor' | 'result' | 'extractParameters'
    title?: string
    status: 'success' | 'error' | 'pending'
    content?: string
  }[]>,
  thoughtProcessId: Annotation<string>,
  // Add the 'ui' field to the state annotation for handling UI messages
  ui: Annotation({ reducer: uiMessageReducer, default: () => [] }),
});

const callChatModal = async (state: typeof StateAnnotation.State) => {
  console.log("ğŸš€ ~ callChatModal ~ state:", state)
  const response = await chatModel.invoke(state.messages);

  return { messages: [response] };
}


const client = new MultiServerMCPClient({
  mcpServers: {
    "mcp-server-chart": {
      "command": "npx",
      "args": [
        "-y",
        "@antv/mcp-server-chart"
      ],
      "env": {
        "DISABLED_TOOLS": "generate_district_map,generate_flow_diagram,generate_mind_map,generate_organization_chart"
      }
    }
  }
})


const pushThoughtProcess = async (items: any[], state: typeof StateAnnotation.State, config: LangGraphRunnableConfig) => {
  const ui = typedUi<any>(config);
  // Find the associated message to link the UI update
  const associatedMessage = state.messages.find(
    (msg) => msg.id === state.thoughtProcessId,
  );
  ui.push({
    id: state.thoughtProcessId,
    name: 'ThoughtProcess',
    props: { items },
    metadata: {
      type: 'header'
    }
  }, { message: associatedMessage });
}


const analysisAgent = async (state: typeof StateAnnotation.State) => {
  
  const model = loadChatModel();
  // Here we only save in-memory
  const memory = new MemorySaver();


  const agent = createReactAgent({
    llm: model,
    tools: [getMarketingPlan, ...(ChartTools)],
    checkpointSaver: memory,
    version: 'v2',
    prompt: await AnalysisAgentPrompt.format({ result_data: JSON.stringify(state.apiResult) })
  });
  // ä¿®å¤ç±»å‹ä¸åŒ¹é…ï¼šagent.invoke æœŸæœ›çš„å‚æ•°ç±»å‹ä¸æ˜¯ BaseMessage[]ï¼Œè€Œæ˜¯ state æˆ– null
  // è¿™é‡Œç›´æ¥ä¼ é€’ state ä»¥ç¬¦åˆç±»å‹è¦æ±‚
  const response = await agent.invoke(
    {
      messages:  state.messages
    }
  );

  // åªè¿”å›æœ€ç»ˆçš„AIåˆ†æç»“æœï¼Œè¿‡æ»¤æ‰ä¸­é—´å¤„ç†æ¶ˆæ¯
  // ä¿ç•™åŸå§‹ç”¨æˆ·æ¶ˆæ¯å’Œæœ€ç»ˆAIå›å¤
  const originalUserMessage = state.messages.find(msg => isHumanMessage(msg));
  const finalAIMessages = response.messages.filter((msg: BaseMessage) => isAIMessage(msg));
  
  // æ„å»ºåªåŒ…å«ç”¨æˆ·æ¶ˆæ¯å’Œæœ€ç»ˆAIå›å¤çš„æ¶ˆæ¯æ•°ç»„
  const filteredMessages: BaseMessage[] = [];
  if (originalUserMessage) {
    filteredMessages.push(originalUserMessage);
  }
  // åªå–æœ€åä¸€æ¡AIæ¶ˆæ¯ä½œä¸ºæœ€ç»ˆç»“æœ
  if (finalAIMessages.length > 0) {
    const lastMessage = finalAIMessages[finalAIMessages.length - 1];
    lastMessage.id = state.thoughtProcessId
    filteredMessages.push(lastMessage);
  }

  return {
    ...response,
    messages: filteredMessages
  }
}

const apiExecutor = async (state: typeof StateAnnotation.State, config: LangGraphRunnableConfig) => {
  console.log("ğŸš€ ~ apiExecutor ~ state:", state)

  const apiInfo = state.apiInfo
  const api = ApiExecutor.getApi(apiInfo.primary_recommendation)
  if (!api) {
    throw new Error(`Api ${apiInfo.primary_recommendation} not found`)
  }

  try {
    void pushThoughtProcess(
      [
        ...(state.thoughtChain ?? []),
        {
          key: 'apiExecutor' as const,
          title: 'æ¥å£æŸ¥è¯¢',
          status: 'pending' as const,
          content: 'æŸ¥è¯¢ä¸­...',
        },
      ],
      state,
      config
    )

    // // Find the associated message to link the UI update
    // const associatedMessage = state.messages.find(
    //   (msg) => msg.id === state.thoughtProcessId,
    // );

    // ui.push({
    //   id: state.thoughtProcessId, // Use the same ID to update the existing UI element
    //   name: 'ThoughtProcess',
    //   props: { items: [
    //     ...(state.thoughtChain ?? []),
    //     {
    //       key: 'apiExecutor' as const,
    //       title: 'æ¥å£æŸ¥è¯¢',
    //       status: 'pending' as const,
    //       content: 'æŸ¥è¯¢ä¸­...',
    //     },
    //   ]},
    // }, { message: associatedMessage });

    const result = await new ApiExecutor(api).execute(apiInfo.api_params)

    const updatedThoughtChain = [
      ...(state.thoughtChain ?? []),
      {
        key: 'apiExecutor' as const,
        title: 'æ¥å£æŸ¥è¯¢',
        status: 'success' as const,
        content: `æ¥å£æŸ¥è¯¢æˆåŠŸï¼Œç»“æœä¸ºï¼š\n\`\`\`json\n${JSON.stringify(
          result,
          null,
          2,
        )}\n\`\`\``,
      },
      {
        key: 'done',
        title: 'å®Œæˆ',
        status: 'success' as const,
        content: 'åˆ†ææ€è€ƒå®Œæˆï¼',
      }
    ];

    void pushThoughtProcess(
      updatedThoughtChain,
      state,
      config
    )

    return {
      secondExtract: null,
      apiResult: result,
      thoughtChain: updatedThoughtChain,
    }

  } catch (error) {
    if (error instanceof z.ZodError) {
      const userInput: string = interrupt({
        questions: error.issues.map(it => `${it.path}:${it.message}`),
      });

      console.log("ğŸš€ ~ ApiExecutor ~ execute ~ userInput:", z.toJSONSchema(z.object(api.parameters)))

      return {
        secondExtract: {
          sourceState: apiInfo.api_params ?? {},
          schema: z.toJSONSchema(z.object(api.parameters)),
          userInput
        }
      }
    }
  }

}

const analysisOfIntentions = async (state: typeof StateAnnotation.State, config: LangGraphRunnableConfig) => {
  console.log("ğŸš€ ~ analysisOfIntentions ~ state:", state.secondExtract)
  const ui = typedUi<any>(config);

  const parser = new JsonOutputParser<AnalysisOfIntentionsOutput>();

  const response = await AnalysisOfIntentionsPrompt
    .pipe(analysisModal)
    .pipe(parser)
    .invoke({
      format_instructions: z.toJSONSchema(structuredOutput),
      system_time: new Date(),
      api_list: apis.map(it => ({
        name: it.name,
        description: it.description,
        parameters: it.parameters ? z.toJSONSchema(z.object(it.parameters)) : undefined,
        response: it.response ? z.toJSONSchema(z.object(it.response)) : undefined,
      })),
      user_query: state.messages[0] as HumanMessage,
    });

  console.log('response', response)

  const thoughtProcessId = uuidv4()

  const initialThoughtChain = [
    {
      key: 'analysisOfIntentions' as const,
      title: 'ç”¨æˆ·æ„å›¾åˆ†æ',
      content: response.analysis_details,
      status: 'success' as const,
    },
    {
      key: 'matchApis' as const,
      title: 'API åŒ¹é…',
      status: 'success' as const,
      content: `
        åŒ¹é…æˆåŠŸAPIï¼š ${response.matched_apis?.[0]?.api_name};
        åŒ¹é…åŸå› ï¼š ${response.matched_apis?.[0]?.match_reason};
        åŒ¹é…å¾—åˆ†ï¼š ${response.matched_apis?.[0]?.relevance_score};
      `,
    }
  ];

  // Create the AIMessage first to establish a link
  const aiMessage = new AIMessage({
    content: 'æˆ‘æ­£åœ¨åˆ†ææ‚¨çš„è¯·æ±‚...',
    id: thoughtProcessId,
  }, {
    status: 'pending',
  });

  // Push the initial state of the thought process UI and associate it with the AIMessage
  ui.push(
    {
      id: thoughtProcessId,
      name: 'ThoughtProcess',
      props: { items: initialThoughtChain },
      metadata: {
        type: 'header'
      }
    },
    { message: aiMessage },
  );

  return {
    // We no longer send the thought chain in the AIMessage content.
    // Instead, we send a simple introductory message.
    messages: [aiMessage],
    apiInfo: response,
    thoughtProcessId,
    thoughtChain: initialThoughtChain,
  };
}

const done = async (state: typeof StateAnnotation.State, config: LangGraphRunnableConfig) => {
  const ui = typedUi<any>(config);
  const doneMessage = new SystemMessage({
    content: '',
    id: 'done:' + uuidv4(),
    response_metadata: {
      status: 'finished',
    }
  }, {
    status: 'success',
  });
  ui.push(
    {
      name: 'DoneDivider',
      props: { },
      metadata: {
        type: 'content'
      }
    },
    { message: doneMessage }
  );
  return {
    messages: doneMessage,
  }
}

const extractParameters = async (state: typeof StateAnnotation.State, config: LangGraphRunnableConfig) => {
  const ui = typedUi<any>(config);

  if (!state.secondExtract) {
    return new Command({
      goto: "analysisAgent",
    })
  }

  const { schema, userInput } = state.secondExtract ?? {}

  const parser = new JsonOutputParser<AnalysisOfIntentionsOutput>();
  const response = await (await ExtractParametersPrompt)
    .pipe(analysisModal)
    .pipe(parser)
    .invoke({
      format_instructions: schema,
      system_time: new Date(),
      user_input: userInput,
    });
  console.log("ğŸš€ ~ extractParameters ~ response:", response)

  const params = {
    ...state.apiInfo.api_params,
    ...(response ?? {}),
  }

  const updatedThoughtChain = [
    ...(state.thoughtChain ?? []).filter(
      (step) => step.key !== 'apiExecutor', // Remove the old pending/failed apiExecutor step
    ),
    {
      key: 'extractParameters' as const,
      title: 'å‚æ•°æå–',
      status: 'success' as const,
      content: `å‚æ•°æå–æˆåŠŸï¼Œå‚æ•°ä¸ºï¼š\n\`\`\`json\n${JSON.stringify(
        params,
        null,
        2,
      )}\n\`\`\``,
    },
  ];

  // Find the associated message to link the UI update
  const associatedMessage = state.messages.find(
    (msg) => msg.id === state.thoughtProcessId,
  );

  // Push an update to the ThoughtProcess UI component
  ui.push({
    id: state.thoughtProcessId, // Use the same ID to update
    name: 'ThoughtProcess',
    props: { items: updatedThoughtChain },
    metadata: {
      type: 'header'
    }
  }, { message: associatedMessage });

  return new Command({
    goto: "apiExecutor",
    update: {
      // The AIMessage is no longer needed to display the thought process
      apiInfo: {
        ...state.apiInfo,
        api_params: params,
      },
      thoughtChain: updatedThoughtChain,
    }
  })
  
}

// const fetchAgent = async (state: typeof MessagesAnnotation.State) => {

//   const prompt = await fetchAgentPrompt.format({
//     system_time: new Date().toISOString(),
//   });

//   const agent = createReactAgent({
//     llm: chatModel,
//     prompt,
//     tools: [],
//   });

//   const response = await agent.invoke(state);

//   return { messages: [response] };
// }


// const canvasAgent = async () => {
  
// }

// Define a new graph. We use the prebuilt MessagesAnnotation to define state:
// https://langchain-ai.github.io/langgraphjs/concepts/low_level/#messagesannotation
const workflow = new StateGraph(StateAnnotation, ConfigurationSchema)
  .addNode("analysisAgent", analysisAgent)
  // Define the two nodes we will cycle between
  // .addNode("fetchAgent", fetchAgent)
  // .addNode("canvasAgent", canvasAgent)
  .addNode("analysisOfIntentions", analysisOfIntentions)
  .addNode("apiExecutor", apiExecutor)
  .addNode("done", done)
  .addNode("extractParameters", extractParameters, { ends: [
    "analysisAgent",
    "apiExecutor"
  ] })
  // Set the entrypoint as `callModel`
  // This means that this node is the first one called
  .addEdge(START, "analysisOfIntentions")
  .addEdge("analysisOfIntentions", "apiExecutor")
  .addEdge("apiExecutor", "extractParameters")
  .addEdge("analysisAgent", "done")
  .addEdge("done", END);

  // // This means that after `tools` is called, `callModel` node is called next.
  // .addEdge("analysisOfIntentions", "__end__");

// Finally, we compile it!
// This compiles it into a graph you can invoke and deploy.
export const graph = workflow.compile({
  interruptBefore: [], // if you want to update the state before calling the tools
  interruptAfter: [],
});
